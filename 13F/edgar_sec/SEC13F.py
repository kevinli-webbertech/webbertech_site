import sys
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
import requests
import pandas as pd
from bs4 import BeautifulSoup
import platform
from functools import lru_cache
import time
from SEC13F_util import *

class SEC13F:
    __headers__ = {
        "User-Agent": "test@gmail.com"
    }

    __LINUX_CHROME_DRIVER_PATH__ = '/usr/bin/chromedriver'

    def __init__(self):
        pd.set_option('display.max_rows', None)
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.max_colwidth', None)


    # Check if the system is Linux
    @lru_cache(maxsize=256)
    def __is_linux(self):
        if platform.system() == "Linux":
            return True
        else:
            return False

    """
    param: cik_key (https://www.sec.gov/files/company_tickers.json)
    return: the source code of the url of the following,
     
    https://www.sec.gov/edgar/browse/?cik=1350694
    
    For invert and forward search of company name using Edgar Search please refer to the URLs in the README.md in this folder.
    """

    @lru_cache(maxsize=256)
    def __get_page_source(self, cik_key):
        # required to access sec gov pages (via documentation)
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument('user-data-dir=/tmp/chrome_headless')
        chrome_options.add_argument("start-maximized")
        chrome_options.add_argument("disable-infobars")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument(
            "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36")

        if self.__is_linux():
            driver = webdriver.Chrome(service=Service(self.__LINUX_CHROME_DRIVER_PATH__), options=chrome_options)
        else:
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)

        driver.get(f"https://www.sec.gov/edgar/browse/?cik={cik_key}")
        driver.implicitly_wait(20)

        page_source = driver.page_source
        driver.quit()
        return page_source

    """
    finds first htm link on the sec gov page and then fetches it.
    It should be the latest 13F-HR filing.
    """
    def __find_htm_link(self, soup):
        data_export_div = soup.find("div",
                                    {"data-export": "Quarterly report filed by institutional managers, Holdings "})
        if data_export_div is None:
            raise Exception("Div with the data-export attribute not found.")

        a_tag = data_export_div.find_all("a", class_="filing-link-all-files")
        if a_tag:
            href = a_tag[0].get("href")
            filing_url = "https://sec.gov" + href
            print("Found the .htm link:", filing_url)
            return filing_url
        else:
            print("No .htm link found within the div.")

    """ 
    param: cik_key
    return: top holdings of the specified company based on the xml from SEC website.
    Example: https://www.sec.gov/Archives/edgar/data/1350694/000117266125000823/infotable.xml
    """
    def find_stock_holdings(self, cik_key):
        page_source = self.__get_page_source(cik_key)
        if page_source is None:
            raise Exception("can't grab the initial html page.")

        soup = BeautifulSoup(page_source, "html.parser")

        response = requests.get(self.__find_htm_link(soup), headers=self.__headers__)

        if not response.status_code == 200:
            raise Exception("Failed to get the filing page")
        soup = BeautifulSoup(response.text, 'html.parser')

        date_div = soup.find('div', class_='formGrouping')
        filing_date = date_div.find('div', class_='info').get_text().strip()

        links = soup.find_all('a', href=True)
        # eg:
        # `url: https://www.sec.gov/Archives/edgar/data/1350694/000117266125000823/0001172661-25-000823-index.htm`
        infotable_links = [link.get('href') for link in links if link.get('href').endswith('.xml')]

        if not infotable_links or len(infotable_links) == 0:
            raise Exception("No XML files found.")

        # eg:
        # https://www.sec.gov/Archives/edgar/data/1350694/000117266125000823/infotable.xml
        latest_13f_filing_xml = infotable_links[-1]

        ### TODO: Need to break down from here to the end of this function and put it into aggregation_from_sec_xml(xml_url, filing_date)
        xml_doc_url = f"https://www.sec.gov{latest_13f_filing_xml}"

        print("\nlatest_13f_filing XML url:", xml_doc_url)

        xml_response = requests.get(xml_doc_url, headers=self.__headers__)
        xml_content = xml_response.text
        soup = BeautifulSoup(xml_content, 'xml')
        info_tables = soup.find_all('infoTable')

        data = []
        for info in info_tables:
            company_name = info.find('nameOfIssuer').text if info.find('nameOfIssuer') else 'None'
            total_value = int(info.find('value').text) if info.find('value') else 0
            shares_table = info.find('shrsOrPrnAmt')
            shares_owned = int(shares_table.find('sshPrnamt').text) if shares_table and shares_table.find('sshPrnamt') else 0

            data.append({'Company Name': company_name,
                        "Total Value": total_value,
                        "Shares Owned": shares_owned,
                        "Filing Date": filing_date
                        })

        df = pd.DataFrame(data)
        df = df.groupby('Company Name', as_index=False).agg({
            'Total Value': 'sum',
            'Shares Owned': 'sum',
            'Filing Date': 'first'
        })

        df.sort_values(by='Total Value', ascending=False, inplace=True)

        df['Total Value'] = df['Total Value'].astype(str)
        df['Shares Owned'] = df['Shares Owned'].astype(str)

        for index, row in df.iterrows():
            total_value = int(row['Total Value'])
            shares_owned = int(row['Shares Owned'])

            df.at[index, 'Total Value'] = divisibleBy(total_value)
            df.at[index, 'Shares Owned'] = divisibleBy(shares_owned)

        return df


    """
    TODO: 
        Darren please try this, try to cut the code logic from the bulky logic from the original line 121 to 164
    parameter: https://www.sec.gov/Archives/edgar/data/1350694/000117266125000823/infotable.xml
    return: formated output
    """
    def aggregation_from_sec_xml(self, xml_url, filing_date):
        xml_response = requests.get(xml_url, headers=self.__headers__)
        print(xml_response.text)

        #TODO xml_to_pandas

        #df = pd.read_xml(xml_url)
        #df_grouped = df.groupby("name")[["value", "shrsOrPrnAmt"]].sum().reset_index()
        # Display the top 5 holdings

        #df_grouped = df_grouped.sort_values(by="value", ascending=False).head()
        #print(df_grouped)

    """"
    just lookup for one company name to cik.
    """
    def cik_lookup(self, company_name):
        print(company_name)

        # 1350694
        company_tickers = requests.get("https://www.sec.gov/files/company_tickers.json",headers=self.__headers__)

        company_data = pd.DataFrame.from_dict(company_tickers.json(), orient='index')

        #fill all ciks with leading zeros for proper cik key 
        company_data['cik_str'] = company_data['cik_str'].astype(str).str.zfill(10)

        #check to see if its one word (ticker) or multiple words (the company title name)
        if len(company_name.split()) == 1:
            company_name = company_name.upper().strip()

            if not company_data[company_data['ticker'] == company_name].empty:
                #grabs first cik_str of row it sees
                cik_str = company_data[company_data['ticker'] == company_name]['cik_str'].iloc[0]
                return cik_str
            else:
                raise Exception(f"Ticker {company_name} not found.")

        ## case sensitivity check, use "Apple Inc" vs "Apple INC"
        else:
            if not company_data[company_data['title'] == company_name.strip()].empty:
                #grabs first cik_str of row it sees
                cik_str = company_data[company_data['title'] == company_name.strip()]['cik_str'].iloc[0]
                return cik_str
            else:
                print(f"Company {company_name} not found.")

    ## Darren can you see if you can fill this up
    def company_name_lookup(company_name:str):
        pass

    """
    Finds overlapping stocks based on *args number of stock dataframes placed into the parameter
    Make sure you use the find_stock_holdings() function to generate stock holding dataframes and 
    input them into this function.
    
    Example: 
        c = SEC13F()
        c.find_common_holdings_multi_cik(tuple(['1350694', '1067983', '1037389', '1610520']))
        
    Output: A list of company names, displayed vertically.
    """
    def find_common_holdings_multi_cik(self, list_of_ciks):
        if len(list_of_ciks) == 0:
            raise Exception("Invalid input, please check function definitions.")

        data_frames = []
        for cik in list_of_ciks:
           #company_name = self.company_name_lookup(cik)
           data_frames.append(self.find_stock_holdings(cik))
        same_holdings = aggregate_holdings(data_frames)

        # TODO Print all original names of the company from CIK
        # company_names = cik_lookup(list_of_ciks)

        print("\n------------------Shared Stock Holdings---------------------")
        shared_holdings = same_holdings.split(",")
        for holding in shared_holdings:
            print(holding.strip())
        print("\n------------------End Stock Holdings---------------------")


if __name__ == "__main__":
    c = SEC13F()
    companies = ['BHLB','Apple Inc.','UBS','META','COST',"AMERICAN EXPRESS CO","ABBOTT LABORATORIES "]
    for company in companies:
        print(c.cik_lookup(company))

    start = time.time()
    c.find_common_holdings_multi_cik(tuple(['1350694', '1067983', '1037389', '1610520']))
    end = time.time()
    print("function timing test:"+ str(end - start))


    #c.aggregation_from_sec_xml("https://www.sec.gov/Archives/edgar/data/1350694/000117266125000823/infotable.xml")
